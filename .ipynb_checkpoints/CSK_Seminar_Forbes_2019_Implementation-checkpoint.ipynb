{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9JZ5Kr-Wk1dC"
   },
   "source": [
    "### ***Physical Common Sense Knowledge***\n",
    "\n",
    "<u>Author:</u> <br/>\n",
    "Awantee Deshpande, <br/>\n",
    "MS Computer Science <br/>\n",
    "Saarland University <br/> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hw6PYjrEl3Tk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2sU6hnVmkho"
   },
   "source": [
    "This implementation is a part of the seminar conducted by MPI-Informatics Saarland, titled **Commonsense Knowledge Extraction and Curation (Winter Semester 2020/21)**. <br/>\n",
    "\n",
    "The notebook provides the steps to reproduce the results of the paper \"Do Neural Language Representations Learn Physical Commonsense?\" by Forbes et al.(2019) [https://arxiv.org/pdf/1908.02899.pdf]. \n",
    "\n",
    "The codebase requires a Python 3.7+ environment. This implementation has been slightly restructured to work in a Google Colab environment. The implementation takes a few hours to run in a GPU runtime.\n",
    "\n",
    "The main git repository by Forbes et al. is available at\n",
    "https://github.com/mbforbes/physical-commonsense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "atYIxUKLlxyE"
   },
   "source": [
    "Step I: Clone the original git repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yNc5rmmcwBDd",
    "outputId": "ed962c54-e09c-43b6-d4ec-d493d396301f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'physical-commonsense'...\n",
      "remote: Enumerating objects: 127, done.\u001b[K\n",
      "remote: Total 127 (delta 0), reused 0 (delta 0), pack-reused 127\u001b[K\n",
      "Receiving objects: 100% (127/127), 6.31 MiB | 4.79 MiB/s, done.\n",
      "Resolving deltas: 100% (26/26), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/mbforbes/physical-commonsense.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zBgM79D3yCbZ"
   },
   "source": [
    "Step II: Install the requirements for the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5GKO3w3xEjb"
   },
   "outputs": [],
   "source": [
    "!pip install -r \"/content/physical-commonsense/requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZXEdqGzryG9a"
   },
   "source": [
    "Step III: Retrieve external data. (The main data is already in subfolders of data/; this is for larger blobs like GloVe.) This script also creates some other directories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qz5rmVo2xnlh",
    "outputId": "f7073b5a-a098-4c76-e96d-24bc0a7c0db0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ set -e\n",
      "+ mkdir -p data/glove/\n",
      "+ '[' '!' -f data/glove/vocab-pc.glove.840B.300d.txt.npz ']'\n",
      "+ curl https://homes.cs.washington.edu/~mbforbes/physical-commonsense/vocab-pc.glove.840B.300d.txt.npz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1495k  100 1495k    0     0   965k      0  0:00:01  0:00:01 --:--:--  965k\n",
      "+ mkdir -p data/dep-embs/\n",
      "+ '[' '!' -f data/dep-embs/vocab-pc.dep-embs.npz ']'\n",
      "+ curl https://homes.cs.washington.edu/~mbforbes/physical-commonsense/vocab-pc.dep-embs.npz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2307k  100 2307k    0     0  1640k      0  0:00:01  0:00:01 --:--:-- 1639k\n",
      "+ mkdir -p data/elmo/\n",
      "+ '[' '!' -f data/elmo/sentences.elmo.npz ']'\n",
      "+ curl https://homes.cs.washington.edu/~mbforbes/physical-commonsense/sentences.elmo.npz\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  705M  100  705M    0     0  22.1M      0  0:00:31  0:00:31 --:--:-- 23.3M\n",
      "+ mkdir -p data/results/graphs/\n"
     ]
    }
   ],
   "source": [
    "!\"/content/physical-commonsense/scripts/get_data.sh\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lyk36zein2h5"
   },
   "source": [
    "In Google Colab, the data folder is created outside the cloned physical-commonsense repository. The directories inside it have to be manually moved to the data folder in the physical-commonsense repo. \n",
    "\n",
    "This can be done in the Colab environment by dragging and dropping the folders dep-embs, elmo, glove, and results into physical-commonsense/data. \n",
    "\n",
    "The results and graphs get accumulated in the physical-commonsense/data/results folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TwnXcpKvTsy"
   },
   "source": [
    "Step IV: Run random and majority baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rT8IDS91JtMq",
    "outputId": "395b7b95-6826-43e8-d3da-d76a1d60e836"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/physical-commonsense\n",
      "02/04/2021 03:22:22 PM INFO: Running random baseline for Abstract_ObjectsProperties\n",
      "02/04/2021 03:22:22 PM INFO: \tAcc: 0.505, Micro F1: 0.257, object macro F1: 0.257, property macro F1: 0.256\n",
      "02/04/2021 03:22:22 PM INFO: Running random baseline for Situated_ObjectsProperties\n",
      "02/04/2021 03:22:23 PM INFO: \tAcc: 0.500, Micro F1: 0.229, object macro F1: 0.249, property macro F1: 0.262\n",
      "02/04/2021 03:22:23 PM INFO: Running random baseline for Situated_ObjectsAffordances\n",
      "02/04/2021 03:22:23 PM INFO: \tAcc: 0.486, Micro F1: 0.480, object macro F1: 0.473, affordance macro F1: 0.551\n",
      "02/04/2021 03:22:23 PM INFO: Running random baseline for Situated_AffordancesProperties\n",
      "02/04/2021 03:22:23 PM INFO: NumExpr defaulting to 2 threads.\n",
      "02/04/2021 03:22:24 PM INFO: \tAcc: 0.503, Micro F1: 0.229, affordance macro F1: 0.241, property macro F1: 0.258\n",
      "02/04/2021 03:22:24 PM INFO: \n",
      "02/04/2021 03:22:24 PM INFO: Running majority: by category baseline for Abstract_ObjectsProperties\n",
      "02/04/2021 03:22:25 PM INFO: \tAcc: 0.843, Micro F1: 0.308, object macro F1: 0.337, property macro F1: 0.113\n",
      "02/04/2021 03:22:25 PM INFO: Running majority: by category baseline for Situated_ObjectsProperties\n",
      "02/04/2021 03:22:25 PM INFO: \tAcc: 0.859, Micro F1: 0.167, object macro F1: 0.156, property macro F1: 0.047\n",
      "02/04/2021 03:22:25 PM INFO: Running majority: by category baseline for Situated_ObjectsAffordances\n",
      "02/04/2021 03:22:25 PM INFO: \tAcc: 0.818, Micro F1: 0.824, object macro F1: 0.822, affordance macro F1: 0.678\n",
      "02/04/2021 03:22:25 PM INFO: Running majority: by category baseline for Situated_AffordancesProperties\n",
      "02/04/2021 03:22:27 PM INFO: \tAcc: 0.859, Micro F1: 0.167, affordance macro F1: 0.178, property macro F1: 0.047\n",
      "02/04/2021 03:22:27 PM INFO: \n",
      ",Abstract,,Situated,,,,,\n",
      ",OP,,OP,,OA,,AP,\n",
      "method,obj,prop,obj,prop,obj,aff,aff,prop\n",
      "random,0.26,0.26,0.25,0.26,0.47,0.55,0.24,0.26\n",
      "majority: by category,0.34,0.11,0.16,0.05,0.82,0.68,0.18,0.05\n"
     ]
    }
   ],
   "source": [
    "%cd physical-commonsense/\n",
    "!python -m pc.baselines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zR797QB5vXTu"
   },
   "source": [
    "Step V: Run GloVe, Dependency Embeddings, and ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J_IxBzb9PCp4",
    "outputId": "17b6af81-03d7-4b57-9c31-23a15293e000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04/2021 03:23:34 PM INFO: Running train+test for Abstract_ObjectsProperties, Glove\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 19. Train acc: 83.54. Train loss: 0.1646\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 39. Train acc: 83.22. Train loss: 0.1527\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 59. Train acc: 86.39. Train loss: 0.1042\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 79. Train acc: 89.45. Train loss: 0.0831\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 99. Train acc: 90.66. Train loss: 0.0752\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 119. Train acc: 90.87. Train loss: 0.0728\n",
      "02/04/2021 03:23:43 PM INFO: Epoch 139. Train acc: 91.09. Train loss: 0.0715\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 159. Train acc: 91.22. Train loss: 0.0708\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 179. Train acc: 91.27. Train loss: 0.0704\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 199. Train acc: 91.33. Train loss: 0.0702\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 219. Train acc: 91.37. Train loss: 0.0700\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 239. Train acc: 91.39. Train loss: 0.0699\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 259. Train acc: 91.41. Train loss: 0.0698\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 279. Train acc: 91.41. Train loss: 0.0697\n",
      "02/04/2021 03:23:44 PM INFO: Epoch 299. Train acc: 91.43. Train loss: 0.0697\n",
      "02/04/2021 03:23:45 PM INFO: \tAcc: 0.897, Micro F1: 0.617, object macro F1: 0.622, property macro F1: 0.459\n",
      "02/04/2021 03:23:45 PM INFO: Running train+test for Situated_ObjectsProperties, Glove\n",
      "02/04/2021 03:23:45 PM INFO: Epoch 19. Train acc: 84.87. Train loss: 0.1513\n",
      "02/04/2021 03:23:46 PM INFO: Epoch 39. Train acc: 84.78. Train loss: 0.1453\n",
      "02/04/2021 03:23:46 PM INFO: Epoch 59. Train acc: 88.94. Train loss: 0.0884\n",
      "02/04/2021 03:23:46 PM INFO: Epoch 79. Train acc: 90.96. Train loss: 0.0704\n",
      "02/04/2021 03:23:46 PM INFO: Epoch 99. Train acc: 91.53. Train loss: 0.0674\n",
      "02/04/2021 03:23:46 PM INFO: Epoch 119. Train acc: 91.66. Train loss: 0.0666\n",
      "02/04/2021 03:23:47 PM INFO: Epoch 139. Train acc: 91.73. Train loss: 0.0661\n",
      "02/04/2021 03:23:47 PM INFO: Epoch 159. Train acc: 91.74. Train loss: 0.0659\n",
      "02/04/2021 03:23:47 PM INFO: Epoch 179. Train acc: 91.77. Train loss: 0.0657\n",
      "02/04/2021 03:23:47 PM INFO: Epoch 199. Train acc: 91.86. Train loss: 0.0656\n",
      "02/04/2021 03:23:48 PM INFO: Epoch 219. Train acc: 91.87. Train loss: 0.0656\n",
      "02/04/2021 03:23:48 PM INFO: Epoch 239. Train acc: 91.86. Train loss: 0.0655\n",
      "02/04/2021 03:23:48 PM INFO: Epoch 259. Train acc: 91.85. Train loss: 0.0655\n",
      "02/04/2021 03:23:48 PM INFO: Epoch 279. Train acc: 91.85. Train loss: 0.0655\n",
      "02/04/2021 03:23:48 PM INFO: Epoch 299. Train acc: 91.87. Train loss: 0.0655\n",
      "02/04/2021 03:23:49 PM INFO: \tAcc: 0.903, Micro F1: 0.577, object macro F1: 0.564, property macro F1: 0.393\n",
      "02/04/2021 03:23:49 PM INFO: Running train+test for Situated_ObjectsAffordances, Glove\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 19. Train acc: 73.11. Train loss: 0.2452\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 39. Train acc: 81.99. Train loss: 0.1501\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 59. Train acc: 89.41. Train loss: 0.0949\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 79. Train acc: 92.42. Train loss: 0.0697\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 99. Train acc: 94.44. Train loss: 0.0602\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 119. Train acc: 94.60. Train loss: 0.0582\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 139. Train acc: 94.85. Train loss: 0.0570\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 159. Train acc: 94.95. Train loss: 0.0566\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 179. Train acc: 94.99. Train loss: 0.0564\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 199. Train acc: 94.99. Train loss: 0.0562\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 219. Train acc: 94.99. Train loss: 0.0561\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 239. Train acc: 94.99. Train loss: 0.0561\n",
      "02/04/2021 03:23:49 PM INFO: Epoch 259. Train acc: 94.99. Train loss: 0.0560\n",
      "02/04/2021 03:23:50 PM INFO: Epoch 279. Train acc: 94.99. Train loss: 0.0560\n",
      "02/04/2021 03:23:50 PM INFO: Epoch 299. Train acc: 94.99. Train loss: 0.0560\n",
      "02/04/2021 03:23:50 PM INFO: \tAcc: 0.852, Micro F1: 0.855, object macro F1: 0.854, affordance macro F1: 0.712\n",
      "02/04/2021 03:23:50 PM INFO: Running train+test for Situated_AffordancesProperties, Glove\n",
      "02/04/2021 03:23:50 PM INFO: NumExpr defaulting to 2 threads.\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "02/04/2021 03:23:53 PM INFO: Epoch 19. Train acc: 84.19. Train loss: 0.1321\n",
      "02/04/2021 03:23:54 PM INFO: Epoch 39. Train acc: 86.53. Train loss: 0.1010\n",
      "02/04/2021 03:23:54 PM INFO: Epoch 59. Train acc: 87.12. Train loss: 0.0948\n",
      "02/04/2021 03:23:55 PM INFO: Epoch 79. Train acc: 87.12. Train loss: 0.0944\n",
      "02/04/2021 03:23:56 PM INFO: Epoch 99. Train acc: 87.13. Train loss: 0.0943\n",
      "02/04/2021 03:23:56 PM INFO: Epoch 119. Train acc: 87.12. Train loss: 0.0943\n",
      "02/04/2021 03:23:57 PM INFO: Epoch 139. Train acc: 87.13. Train loss: 0.0943\n",
      "02/04/2021 03:23:58 PM INFO: Epoch 159. Train acc: 87.12. Train loss: 0.0942\n",
      "02/04/2021 03:23:58 PM INFO: Epoch 179. Train acc: 87.13. Train loss: 0.0942\n",
      "02/04/2021 03:23:59 PM INFO: Epoch 199. Train acc: 87.13. Train loss: 0.0942\n",
      "02/04/2021 03:24:00 PM INFO: Epoch 219. Train acc: 87.14. Train loss: 0.0942\n",
      "02/04/2021 03:24:00 PM INFO: Epoch 239. Train acc: 87.15. Train loss: 0.0942\n",
      "02/04/2021 03:24:01 PM INFO: Epoch 259. Train acc: 87.14. Train loss: 0.0942\n",
      "02/04/2021 03:24:02 PM INFO: Epoch 279. Train acc: 87.15. Train loss: 0.0942\n",
      "02/04/2021 03:24:02 PM INFO: Epoch 299. Train acc: 87.15. Train loss: 0.0942\n",
      "02/04/2021 03:24:03 PM INFO: \tAcc: 0.868, Micro F1: 0.288, affordance macro F1: 0.267, property macro F1: 0.129\n",
      "02/04/2021 03:24:03 PM INFO: \n",
      "02/04/2021 03:24:03 PM INFO: Running train+test for Abstract_ObjectsProperties, DepEmbs\n",
      "02/04/2021 03:24:03 PM INFO: Epoch 19. Train acc: 83.54. Train loss: 0.1646\n",
      "02/04/2021 03:24:03 PM INFO: Epoch 39. Train acc: 83.27. Train loss: 0.1562\n",
      "02/04/2021 03:24:03 PM INFO: Epoch 59. Train acc: 86.27. Train loss: 0.1063\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 79. Train acc: 88.73. Train loss: 0.0868\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 99. Train acc: 90.40. Train loss: 0.0778\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 119. Train acc: 90.66. Train loss: 0.0755\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 139. Train acc: 90.90. Train loss: 0.0742\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 159. Train acc: 90.98. Train loss: 0.0736\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 179. Train acc: 91.02. Train loss: 0.0733\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 199. Train acc: 91.03. Train loss: 0.0731\n",
      "02/04/2021 03:24:04 PM INFO: Epoch 219. Train acc: 91.05. Train loss: 0.0730\n",
      "02/04/2021 03:24:05 PM INFO: Epoch 239. Train acc: 91.07. Train loss: 0.0729\n",
      "02/04/2021 03:24:05 PM INFO: Epoch 259. Train acc: 91.08. Train loss: 0.0728\n",
      "02/04/2021 03:24:05 PM INFO: Epoch 279. Train acc: 91.09. Train loss: 0.0728\n",
      "02/04/2021 03:24:05 PM INFO: Epoch 299. Train acc: 91.10. Train loss: 0.0727\n",
      "02/04/2021 03:24:05 PM INFO: \tAcc: 0.893, Micro F1: 0.607, object macro F1: 0.626, property macro F1: 0.428\n",
      "02/04/2021 03:24:05 PM INFO: Running train+test for Situated_ObjectsProperties, DepEmbs\n",
      "02/04/2021 03:24:06 PM INFO: Epoch 19. Train acc: 84.87. Train loss: 0.1513\n",
      "02/04/2021 03:24:06 PM INFO: Epoch 39. Train acc: 83.58. Train loss: 0.1534\n",
      "02/04/2021 03:24:06 PM INFO: Epoch 59. Train acc: 87.82. Train loss: 0.0934\n",
      "02/04/2021 03:24:06 PM INFO: Epoch 79. Train acc: 90.35. Train loss: 0.0735\n",
      "02/04/2021 03:24:07 PM INFO: Epoch 99. Train acc: 91.31. Train loss: 0.0692\n",
      "02/04/2021 03:24:07 PM INFO: Epoch 119. Train acc: 91.55. Train loss: 0.0679\n",
      "02/04/2021 03:24:07 PM INFO: Epoch 139. Train acc: 91.58. Train loss: 0.0672\n",
      "02/04/2021 03:24:07 PM INFO: Epoch 159. Train acc: 91.63. Train loss: 0.0669\n",
      "02/04/2021 03:24:08 PM INFO: Epoch 179. Train acc: 91.65. Train loss: 0.0668\n",
      "02/04/2021 03:24:08 PM INFO: Epoch 199. Train acc: 91.68. Train loss: 0.0667\n",
      "02/04/2021 03:24:08 PM INFO: Epoch 219. Train acc: 91.69. Train loss: 0.0666\n",
      "02/04/2021 03:24:08 PM INFO: Epoch 239. Train acc: 91.69. Train loss: 0.0666\n",
      "02/04/2021 03:24:09 PM INFO: Epoch 259. Train acc: 91.71. Train loss: 0.0665\n",
      "02/04/2021 03:24:09 PM INFO: Epoch 279. Train acc: 91.72. Train loss: 0.0665\n",
      "02/04/2021 03:24:09 PM INFO: Epoch 299. Train acc: 91.73. Train loss: 0.0665\n",
      "02/04/2021 03:24:09 PM INFO: \tAcc: 0.898, Micro F1: 0.545, object macro F1: 0.551, property macro F1: 0.372\n",
      "02/04/2021 03:24:09 PM INFO: Running train+test for Situated_ObjectsAffordances, DepEmbs\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 19. Train acc: 65.73. Train loss: 0.3278\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 39. Train acc: 75.88. Train loss: 0.2204\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 59. Train acc: 85.53. Train loss: 0.1100\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 79. Train acc: 91.52. Train loss: 0.0783\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 99. Train acc: 93.52. Train loss: 0.0652\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 119. Train acc: 94.05. Train loss: 0.0615\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 139. Train acc: 94.40. Train loss: 0.0599\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 159. Train acc: 94.50. Train loss: 0.0594\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 179. Train acc: 94.56. Train loss: 0.0590\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 199. Train acc: 94.56. Train loss: 0.0588\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 219. Train acc: 94.56. Train loss: 0.0587\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 239. Train acc: 94.60. Train loss: 0.0585\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 259. Train acc: 94.56. Train loss: 0.0584\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 279. Train acc: 94.62. Train loss: 0.0583\n",
      "02/04/2021 03:24:10 PM INFO: Epoch 299. Train acc: 94.60. Train loss: 0.0583\n",
      "02/04/2021 03:24:10 PM INFO: \tAcc: 0.841, Micro F1: 0.839, object macro F1: 0.833, affordance macro F1: 0.673\n",
      "02/04/2021 03:24:10 PM INFO: Running train+test for Situated_AffordancesProperties, DepEmbs\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "02/04/2021 03:24:13 PM INFO: Epoch 19. Train acc: 85.34. Train loss: 0.1440\n",
      "02/04/2021 03:24:14 PM INFO: Epoch 39. Train acc: 86.45. Train loss: 0.1007\n",
      "02/04/2021 03:24:15 PM INFO: Epoch 59. Train acc: 87.01. Train loss: 0.0958\n",
      "02/04/2021 03:24:15 PM INFO: Epoch 79. Train acc: 87.05. Train loss: 0.0953\n",
      "02/04/2021 03:24:16 PM INFO: Epoch 99. Train acc: 87.04. Train loss: 0.0951\n",
      "02/04/2021 03:24:17 PM INFO: Epoch 119. Train acc: 87.05. Train loss: 0.0951\n",
      "02/04/2021 03:24:17 PM INFO: Epoch 139. Train acc: 87.05. Train loss: 0.0951\n",
      "02/04/2021 03:24:18 PM INFO: Epoch 159. Train acc: 87.06. Train loss: 0.0950\n",
      "02/04/2021 03:24:19 PM INFO: Epoch 179. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:19 PM INFO: Epoch 199. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:20 PM INFO: Epoch 219. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:21 PM INFO: Epoch 239. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:21 PM INFO: Epoch 259. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:22 PM INFO: Epoch 279. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:22 PM INFO: Epoch 299. Train acc: 87.05. Train loss: 0.0950\n",
      "02/04/2021 03:24:23 PM INFO: \tAcc: 0.867, Micro F1: 0.275, affordance macro F1: 0.260, property macro F1: 0.116\n",
      "02/04/2021 03:24:23 PM INFO: \n",
      "02/04/2021 03:24:23 PM INFO: Running train+test for Abstract_ObjectsProperties, Elmo\n",
      "02/04/2021 03:24:38 PM INFO: Epoch 19. Train acc: 83.54. Train loss: 0.1646\n",
      "02/04/2021 03:24:38 PM INFO: Epoch 39. Train acc: 85.27. Train loss: 0.1402\n",
      "02/04/2021 03:24:38 PM INFO: Epoch 59. Train acc: 86.95. Train loss: 0.1059\n",
      "02/04/2021 03:24:39 PM INFO: Epoch 79. Train acc: 90.77. Train loss: 0.0718\n",
      "02/04/2021 03:24:39 PM INFO: Epoch 99. Train acc: 92.80. Train loss: 0.0614\n",
      "02/04/2021 03:24:39 PM INFO: Epoch 119. Train acc: 93.39. Train loss: 0.0574\n",
      "02/04/2021 03:24:39 PM INFO: Epoch 139. Train acc: 93.76. Train loss: 0.0553\n",
      "02/04/2021 03:24:40 PM INFO: Epoch 159. Train acc: 93.98. Train loss: 0.0543\n",
      "02/04/2021 03:24:40 PM INFO: Epoch 179. Train acc: 94.04. Train loss: 0.0538\n",
      "02/04/2021 03:24:40 PM INFO: Epoch 199. Train acc: 94.10. Train loss: 0.0535\n",
      "02/04/2021 03:24:40 PM INFO: Epoch 219. Train acc: 94.14. Train loss: 0.0532\n",
      "02/04/2021 03:24:41 PM INFO: Epoch 239. Train acc: 94.18. Train loss: 0.0530\n",
      "02/04/2021 03:24:41 PM INFO: Epoch 259. Train acc: 94.18. Train loss: 0.0529\n",
      "02/04/2021 03:24:41 PM INFO: Epoch 279. Train acc: 94.19. Train loss: 0.0528\n",
      "02/04/2021 03:24:42 PM INFO: Epoch 299. Train acc: 94.21. Train loss: 0.0527\n",
      "02/04/2021 03:24:56 PM INFO: \tAcc: 0.904, Micro F1: 0.667, object macro F1: 0.662, property macro F1: 0.543\n",
      "02/04/2021 03:24:56 PM INFO: Running train+test for Situated_ObjectsProperties, Elmo\n",
      "02/04/2021 03:25:11 PM INFO: Epoch 19. Train acc: 84.87. Train loss: 0.1513\n",
      "02/04/2021 03:25:12 PM INFO: Epoch 39. Train acc: 83.67. Train loss: 0.1575\n",
      "02/04/2021 03:25:12 PM INFO: Epoch 59. Train acc: 88.09. Train loss: 0.0895\n",
      "02/04/2021 03:25:13 PM INFO: Epoch 79. Train acc: 92.03. Train loss: 0.0633\n",
      "02/04/2021 03:25:13 PM INFO: Epoch 99. Train acc: 92.64. Train loss: 0.0585\n",
      "02/04/2021 03:25:14 PM INFO: Epoch 119. Train acc: 92.75. Train loss: 0.0572\n",
      "02/04/2021 03:25:15 PM INFO: Epoch 139. Train acc: 92.85. Train loss: 0.0565\n",
      "02/04/2021 03:25:15 PM INFO: Epoch 159. Train acc: 92.86. Train loss: 0.0563\n",
      "02/04/2021 03:25:16 PM INFO: Epoch 179. Train acc: 92.86. Train loss: 0.0562\n",
      "02/04/2021 03:25:16 PM INFO: Epoch 199. Train acc: 92.86. Train loss: 0.0561\n",
      "02/04/2021 03:25:17 PM INFO: Epoch 219. Train acc: 92.85. Train loss: 0.0561\n",
      "02/04/2021 03:25:17 PM INFO: Epoch 239. Train acc: 92.86. Train loss: 0.0561\n",
      "02/04/2021 03:25:18 PM INFO: Epoch 259. Train acc: 92.86. Train loss: 0.0560\n",
      "02/04/2021 03:25:18 PM INFO: Epoch 279. Train acc: 92.86. Train loss: 0.0560\n",
      "02/04/2021 03:25:19 PM INFO: Epoch 299. Train acc: 92.86. Train loss: 0.0560\n",
      "02/04/2021 03:25:33 PM INFO: \tAcc: 0.899, Micro F1: 0.589, object macro F1: 0.570, property macro F1: 0.448\n",
      "02/04/2021 03:25:33 PM INFO: Running train+test for Situated_ObjectsAffordances, Elmo\n",
      "02/04/2021 03:25:47 PM INFO: Epoch 19. Train acc: 72.68. Train loss: 0.2600\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 39. Train acc: 81.44. Train loss: 0.1653\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 59. Train acc: 90.83. Train loss: 0.0807\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 79. Train acc: 92.85. Train loss: 0.0616\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 99. Train acc: 95.95. Train loss: 0.0449\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 119. Train acc: 96.45. Train loss: 0.0407\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 139. Train acc: 96.52. Train loss: 0.0395\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 159. Train acc: 96.54. Train loss: 0.0392\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 179. Train acc: 96.52. Train loss: 0.0391\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 199. Train acc: 96.54. Train loss: 0.0390\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 219. Train acc: 96.52. Train loss: 0.0390\n",
      "02/04/2021 03:25:48 PM INFO: Epoch 239. Train acc: 96.52. Train loss: 0.0389\n",
      "02/04/2021 03:25:49 PM INFO: Epoch 259. Train acc: 96.54. Train loss: 0.0389\n",
      "02/04/2021 03:25:49 PM INFO: Epoch 279. Train acc: 96.52. Train loss: 0.0389\n",
      "02/04/2021 03:25:49 PM INFO: Epoch 299. Train acc: 96.52. Train loss: 0.0389\n",
      "02/04/2021 03:26:02 PM INFO: \tAcc: 0.852, Micro F1: 0.854, object macro F1: 0.839, affordance macro F1: 0.719\n",
      "02/04/2021 03:26:02 PM INFO: Running train+test for Situated_AffordancesProperties, Elmo\n",
      "tcmalloc: large alloc 2010316800 bytes == 0x9dfc2000 @  0x7f1822f131e7 0x7f17d351a41e 0x7f17d356ac2b 0x7f17d356acc8 0x7f17d35fd349 0x7f17d35ffa55 0x50c19e 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x5095c8 0x50a2fd 0x50beb4 0x507be4 0x5161c5 0x50a12f 0x50beb4 0x507be4 0x509900 0x50a2fd 0x50beb4 0x507be4\n",
      "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "02/04/2021 03:26:21 PM INFO: Epoch 19. Train acc: 85.37. Train loss: 0.1461\n",
      "02/04/2021 03:26:23 PM INFO: Epoch 39. Train acc: 87.16. Train loss: 0.0964\n",
      "02/04/2021 03:26:24 PM INFO: Epoch 59. Train acc: 87.62. Train loss: 0.0909\n",
      "02/04/2021 03:26:26 PM INFO: Epoch 79. Train acc: 87.63. Train loss: 0.0905\n",
      "02/04/2021 03:26:27 PM INFO: Epoch 99. Train acc: 87.63. Train loss: 0.0903\n",
      "02/04/2021 03:26:29 PM INFO: Epoch 119. Train acc: 87.63. Train loss: 0.0902\n",
      "02/04/2021 03:26:31 PM INFO: Epoch 139. Train acc: 87.63. Train loss: 0.0902\n",
      "02/04/2021 03:26:32 PM INFO: Epoch 159. Train acc: 87.63. Train loss: 0.0902\n",
      "02/04/2021 03:26:34 PM INFO: Epoch 179. Train acc: 87.63. Train loss: 0.0901\n",
      "02/04/2021 03:26:35 PM INFO: Epoch 199. Train acc: 87.63. Train loss: 0.0901\n",
      "02/04/2021 03:26:37 PM INFO: Epoch 219. Train acc: 87.63. Train loss: 0.0901\n",
      "02/04/2021 03:26:39 PM INFO: Epoch 239. Train acc: 87.64. Train loss: 0.0901\n",
      "02/04/2021 03:26:40 PM INFO: Epoch 259. Train acc: 87.64. Train loss: 0.0901\n",
      "02/04/2021 03:26:42 PM INFO: Epoch 279. Train acc: 87.63. Train loss: 0.0901\n",
      "02/04/2021 03:26:44 PM INFO: Epoch 299. Train acc: 87.63. Train loss: 0.0901\n",
      "02/04/2021 03:26:58 PM INFO: \tAcc: 0.868, Micro F1: 0.336, affordance macro F1: 0.310, property macro F1: 0.176\n",
      "02/04/2021 03:26:58 PM INFO: \n",
      "Glove,0.62,0.46,0.56,0.39,0.85,0.71,0.27,0.13\n",
      "DepEmbs,0.63,0.43,0.55,0.37,0.83,0.67,0.26,0.12\n",
      "Elmo,0.66,0.54,0.57,0.45,0.84,0.72,0.31,0.18\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MwMQ3QFRhJGp"
   },
   "source": [
    "Step VI: Run BERT based experiements on the abstract OP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kURafZAyJHdG",
    "outputId": "1d678331-e6d4-4907-c334-7878f18d8b4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "100% 434/434 [00:00<00:00, 278934.71B/s]\n",
      "100% 1344997306/1344997306 [01:34<00:00, 14198128.52B/s]\n",
      "Loading traning data\n",
      "5 Samples:\n",
      "- canoe/wet: \"A canoe is wet.\"\n",
      "- colander/slimy: \"A colander is slimy.\"\n",
      "- dolphin/sharp: \"A dolphin is sharp.\"\n",
      "- spider/worn_on_feet: \"A spider is worn on feet.\"\n",
      "- toothbrush/hand_held: \"A toothbrush is hand-held.\"\n",
      "Loading tokenizer...\n",
      "100% 231508/231508 [00:00<00:00, 322691.52B/s]\n",
      "Loading test data\n",
      "5 Samples:\n",
      "- building/eaten_in_summer: \"A building is eaten in summer.\"\n",
      "- plate/large: \"A plate is big.\"\n",
      "- buckle/hot: \"A buckle is hot.\"\n",
      "- elk/used_for_killing: \"An elk is used for killing.\"\n",
      "- housefly/used_for_eating: \"A housefly is used for eating.\"\n",
      "Loading tokenizer...\n",
      "Num train optimization steps: 1610\n",
      "Starting epoch 1/5.\n",
      "Batch:   0% 0/322 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Batch: 100% 322/322 [08:35<00:00,  1.60s/it]\n",
      "Average train loss: 0.3471941244384668\n",
      "train accuracy: 49.99659367396594\n",
      "Starting epoch 2/5.\n",
      "Batch: 100% 322/322 [08:34<00:00,  1.60s/it]\n",
      "Average train loss: 0.2295669172686092\n",
      "train accuracy: 47.35630170316302\n",
      "Starting epoch 3/5.\n",
      "Batch: 100% 322/322 [08:35<00:00,  1.60s/it]\n",
      "Average train loss: 0.16095281046962506\n",
      "train accuracy: 46.94540145985401\n",
      "Starting epoch 4/5.\n",
      "Batch: 100% 322/322 [08:35<00:00,  1.60s/it]\n",
      "Average train loss: 0.10288138771363944\n",
      "train accuracy: 46.7407299270073\n",
      "Starting epoch 5/5.\n",
      "Batch: 100% 322/322 [08:34<00:00,  1.60s/it]\n",
      "Average train loss: 0.05502162865281486\n",
      "train accuracy: 46.53343065693431\n",
      "Running eval after 5 epochs.\n",
      "Batch: 100% 54/54 [00:42<00:00,  1.28it/s]\n",
      "Average test loss: 0.32354623067147525\n",
      "test accuracy: 70.03961165048544\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.bert --task \"abstract-OP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iI7yrttwpxww"
   },
   "source": [
    "Step VII: Run BERT on the situated OP dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HFBneuYgJS34",
    "outputId": "117a96d2-80ec-4968-9930-2c88376831b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Loading traning data\n",
      "5 Samples:\n",
      "- baseball_glove/an_animal: \"A baseball glove is an animal.\"\n",
      "- bed/wet: \"A bed is wet.\"\n",
      "- car/hard: \"A car is hard.\"\n",
      "- suitcase/an_animal: \"A suitcase is an animal.\"\n",
      "- wine_glass/smooth: \"A wine glass is smooth.\"\n",
      "Loading tokenizer...\n",
      "Loading test data\n",
      "5 Samples:\n",
      "- sheep/expensive: \"A sheep is expensive.\"\n",
      "- bottle/light_weight: \"A bottle is light.\"\n",
      "- chair/heavy: \"A chair is heavy.\"\n",
      "- dining_table/eaten_in_summer: \"A dining table is eaten in summer.\"\n",
      "- bottle/used_for_transportation: \"A bottle is used for transportation.\"\n",
      "Loading tokenizer...\n",
      "Num train optimization steps: 3200\n",
      "Starting epoch 1/5.\n",
      "Batch:   0% 0/640 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Batch: 100% 640/640 [16:56<00:00,  1.59s/it]\n",
      "Average train loss: 0.32282703471358654\n",
      "train accuracy: 50.52591687041565\n",
      "Starting epoch 2/5.\n",
      "Batch: 100% 640/640 [16:54<00:00,  1.59s/it]\n",
      "Average train loss: 0.4310799991239254\n",
      "train accuracy: 54.312665036674815\n",
      "Starting epoch 3/5.\n",
      "Batch: 100% 640/640 [16:56<00:00,  1.59s/it]\n",
      "Average train loss: 0.4311256302481467\n",
      "train accuracy: 54.312665036674815\n",
      "Starting epoch 4/5.\n",
      "Batch: 100% 640/640 [16:53<00:00,  1.58s/it]\n",
      "Average train loss: 0.43019386677403904\n",
      "train accuracy: 54.312665036674815\n",
      "Starting epoch 5/5.\n",
      "Batch: 100% 640/640 [16:53<00:00,  1.58s/it]\n",
      "Average train loss: 0.42886700949779644\n",
      "train accuracy: 54.31119804400978\n",
      "Running eval after 5 epochs.\n",
      "Batch: 100% 116/116 [01:29<00:00,  1.30it/s]\n",
      "Average test loss: 0.421349851440739\n",
      "test accuracy: 81.53945945945947\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.bert --task \"situated-OP\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IVwzJyPup3_t"
   },
   "source": [
    "Step VIII: Run BERT on the situated OA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "83ub63d9JoE_",
    "outputId": "8faaed5b-505b-4a1b-bf79-082a80eaae1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Loading traning data\n",
      "5 Samples:\n",
      "- motorcycle/adjust: \"He adjusted the motorcycle.\"\n",
      "- oven/train: \"He trained the oven.\"\n",
      "- motorcycle/kiss: \"He kissed the motorcycle.\"\n",
      "- car/deflect: \"He deflected the car.\"\n",
      "- surfboard/jump: \"He jumped the surfboard.\"\n",
      "Loading tokenizer...\n",
      "Loading test data\n",
      "5 Samples:\n",
      "- horse/disembark: \"He disembarked the horse.\"\n",
      "- skateboard/immerse: \"He immersed the skateboard.\"\n",
      "- sheep/splash: \"He splashed the sheep.\"\n",
      "- chair/sit: \"He sat the chair.\"\n",
      "- skateboard/shave: \"He shaved the skateboard.\"\n",
      "Loading tokenizer...\n",
      "Num train optimization steps: 385\n",
      "Starting epoch 1/5.\n",
      "Batch:   0% 0/77 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Batch: 100% 77/77 [02:02<00:00,  1.59s/it]\n",
      "Average train loss: 0.5280586086158939\n",
      "train accuracy: 32.04278728606357\n",
      "Starting epoch 2/5.\n",
      "Batch: 100% 77/77 [02:02<00:00,  1.59s/it]\n",
      "Average train loss: 0.3386150936139922\n",
      "train accuracy: 32.32314588427058\n",
      "Starting epoch 3/5.\n",
      "Batch: 100% 77/77 [02:02<00:00,  1.59s/it]\n",
      "Average train loss: 0.24905139283135158\n",
      "train accuracy: 32.357375713121435\n",
      "Starting epoch 4/5.\n",
      "Batch: 100% 77/77 [02:02<00:00,  1.59s/it]\n",
      "Average train loss: 0.18747503536934762\n",
      "train accuracy: 32.47351263243684\n",
      "Starting epoch 5/5.\n",
      "Batch: 100% 77/77 [02:02<00:00,  1.59s/it]\n",
      "Average train loss: 0.148260540804828\n",
      "train accuracy: 32.32681336593317\n",
      "Running eval after 5 epochs.\n",
      "Batch: 100% 14/14 [00:11<00:00,  1.21it/s]\n",
      "Average test loss: 0.372766334999789\n",
      "test accuracy: 47.62162162162162\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.bert --task \"situated-OA\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQeAkU5Op7d5"
   },
   "source": [
    "Step IX: Run BERT on the situated AP dataset <br/>\n",
    "NOTE: Only 1 epoch here is not to handicap the model; The authors observe that the model overfits and achieves 0.0 F1 score for 2+ epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyTVCPoEJqdt",
    "outputId": "9fa25582-0401-46eb-d1f7-2e2237c8d824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building model...\n",
      "Loading traning data\n",
      "5 Samples:\n",
      "- drive/sharp: \"If you can drive something, then it is sharp.\"\n",
      "- call/edible: \"If you can call something, then it is edible.\"\n",
      "- bake/used_by_children: \"If you can bake something, then it is used by children.\"\n",
      "- bandage/fun: \"If you can bandage something, then it is fun.\"\n",
      "- arrest/used_for_cleaning: \"If you can arrest something, then it is used for cleaning.\"\n",
      "Loading tokenizer...\n",
      "Loading test data\n",
      "5 Samples:\n",
      "- clean/large: \"If you can clean something, then it is big.\"\n",
      "- swing/used_for_holding_things: \"If you can swing something, then it is used for holding things.\"\n",
      "- attach/light_weight: \"If you can attach something, then it is light.\"\n",
      "- aim/decorative: \"If you can aim something, then it is decorative.\"\n",
      "- swing/lives_in_water: \"If you can swing something, then it lives in water.\"\n",
      "Loading tokenizer...\n",
      "Num train optimization steps: 1918\n",
      "Starting epoch 1/1.\n",
      "Batch:   0% 0/1918 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/pytorch_transformers/optimization.py:166: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n",
      "  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n",
      "Batch: 100% 1918/1918 [51:34<00:00,  1.61s/it]\n",
      "Average train loss: 0.4253802927354534\n",
      "train accuracy: 53.907155664221676\n",
      "Running eval after 1 epochs.\n",
      "Batch: 100% 347/347 [04:29<00:00,  1.29it/s]\n",
      "Average test loss: 0.421129104532637\n",
      "test accuracy: 81.67171171171171\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.bert --task \"situated-AP\" --epochs 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJJoj5qove2V"
   },
   "source": [
    "Step X: Display human baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E6Mc_2inu6IX",
    "outputId": "35dc0392-8637-4a51-e390-cfaed8fe68fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/04/2021 06:46:32 PM INFO: Task.Abstract_ObjectsProperties\n",
      "02/04/2021 06:46:32 PM INFO: \tAcc: 0.900, Micro F1: 0.667, object macro F1: 0.779, property macro F1: 0.800\n",
      "02/04/2021 06:46:32 PM INFO: Task.Situated_ObjectsProperties\n",
      "02/04/2021 06:46:32 PM INFO: \tAcc: 0.820, Micro F1: 0.609, object macro F1: 0.701, property macro F1: 0.693\n",
      "02/04/2021 06:46:32 PM INFO: Task.Situated_ObjectsAffordances\n",
      "02/04/2021 06:46:32 PM INFO: \tAcc: 0.780, Micro F1: 0.800, object macro F1: 0.833, affordance macro F1: 0.929\n",
      "02/04/2021 06:46:32 PM INFO: Task.Situated_AffordancesProperties\n",
      "02/04/2021 06:46:32 PM INFO: \tAcc: 0.700, Micro F1: 0.400, affordance macro F1: 0.650, property macro F1: 0.665\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.human"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RsMQ4PWzvnlb"
   },
   "source": [
    "Step XI: Compute statistical significance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nte-2rICu-r_",
    "outputId": "ab872bfc-193e-4e97-993c-a1d21a527e54"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract-OP:\n",
      "- Random: ***\n",
      "- Majority: ***\n",
      "- Glove: ***\n",
      "- DepEmbs: ***\n",
      "- Bert: (base)\n",
      "- Elmo: **\n",
      "\n",
      "situated-OP:\n",
      "- Random: ***\n",
      "- Majority: ***\n",
      "- Glove: ***\n",
      "- DepEmbs: ***\n",
      "- Bert: (base)\n",
      "- Elmo: ***\n",
      "\n",
      "situated-OA:\n",
      "- Random: ***\n",
      "- Majority: ***\n",
      "- Glove: \n",
      "- DepEmbs: **\n",
      "- Bert: (base)\n",
      "- Elmo: \n",
      "\n",
      "situated-AP:\n",
      "- Random: ***\n",
      "- Majority: ***\n",
      "- Glove: ***\n",
      "- DepEmbs: ***\n",
      "- Bert: (base)\n",
      "- Elmo: ***\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python -m pc.significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pINYZEhvskP"
   },
   "source": [
    "Step XII: Convert BERT's output on the situated-AP task to per-category output (for making graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hsrN_bsxvBS0",
    "outputId": "1b242dc6-3347-40af-c4d0-7151b6e45380"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing A results to data/results/Bert-AP-A.txt\n",
      "Writing P results to data/results/Bert-AP-P.txt\n"
     ]
    }
   ],
   "source": [
    "!python -m scripts.perdatum_to_category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOrOoklVvxsS"
   },
   "source": [
    "Step XIII: Produce graphs (also shown in the paper) for analyzing BERT's output on the situated-AP task per-category, as well as comparing performance vs word occurrence in natural language (found in data/nl/). The graphs are written to physical-commonsense/data/results/graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "PA6xaQGGvEse"
   },
   "outputs": [],
   "source": [
    "!python -m pc.graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYJGmQ3Xv3cu"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CSK_Seminar_Forbes_Implementation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
